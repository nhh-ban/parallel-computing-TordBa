control = control_grid(save_pred = TRUE)
)
# Which parameter combination is the best?
rf_tune_result %>%
select_best(metric = "roc_auc")
# Put the best parameters in the workflow
rf_tuned <-
finalize_workflow(
rf_workflow,
parameters = rf_tune_result %>% select_best(metric = "roc_auc")
)
# Fit the model
fitted_rf <-
rf_tuned %>%
fit(data = spam_train)
# Plot the model
rpart.plot(fitted_rf$fit$fit$fit)
# Predict the train and test data
predictions_rf_test <-
fitted_rf %>%
predict(new_data = spam_test,
type = "prob") %>%
mutate(truth = spam_test$spam)
predictions_rf_train <-
fitted_rf %>%
predict(new_data = spam_train,
type = "prob") %>%
mutate(truth = spam_train$spam)
# Calculate the AUC
auc_rf <-
predictions_rf_test %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "test") %>%
bind_rows(predictions_rf_train %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "train")) %>%
mutate(model = "decision_rf")
auc_results <-
rbind(auc_tree, auc_rf)
auc_results
library(tidyverse)
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_bar()
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_col()
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_col(position = "dodge")
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_col(position = "dodge") +
labs(x = "Model", y = "AUC results")
install.packages(xgboost)
install.packages("xgboost")
library(xgboost)
?xgboost
#Specify the decision tree
xg_mod <-
boost_tree(
trees = 100,
tree_depth = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
mtry = tune(),
min_n = tune()) %>%
set_mode("classification") %>%
set_engine("ranger")
#Specify the decision tree
xg_mod <-
boost_tree(
trees = 100,
tree_depth = tune(),
loss_reduction = tune(),
sample_size = tune(),
learn_rate = tune(),
mtry = tune(),
min_n = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost")
# Set up the workflow
xg_workflow <-
workflow() %>%
add_model(xg_mod) %>%
add_recipe(spam_recipe)
# Make a search grid for the k-parameter
xg_grid <-
grid_latin_hypercube(
tree_depth(),
loss_reduction(),
learn_rate(),
mtry(range = c(1, length(names)/2)),
min_n(),
size = 10
)
# Calculate the cross-validated AUC for all the k's in the grid
xg_tune_result <-
tune_grid(
xg_workflow,
resamples = spam_folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE)
)
# Make a search grid for the k-parameter
xg_grid <-
grid_latin_hypercube(
tree_depth(),
loss_reduction(),
sample_size() = sample_prop(),
# Make a search grid for the k-parameter
xg_grid <-
grid_latin_hypercube(
tree_depth(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
mtry(range = c(1, length(names)/2)),
min_n(),
size = 10
)
# Calculate the cross-validated AUC for all the k's in the grid
xg_tune_result <-
tune_grid(
xg_workflow,
resamples = spam_folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE)
)
# Which parameter combination is the best?
xg_tune_result %>%
select_best(metric = "roc_auc")
# Put the best parameters in the workflow
xg_tuned <-
finalize_workflow(
xg_workflow,
parameters = xg_tune_result %>% select_best(metric = "roc_auc")
)
# Fit the model
fitted_xg <-
xg_tuned %>%
fit(data = spam_train)
# Predict the train and test data
predictions_xg_test <-
fitted_xg %>%
predict(new_data = spam_test,
type = "prob") %>%
mutate(truth = spam_test$spam)
predictions_xg_train <-
fitted_xg %>%
predict(new_data = spam_train,
type = "prob") %>%
mutate(truth = spam_train$spam)
# Calculate the AUC
auc_xg <-
predictions_xg_test %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "test") %>%
bind_rows(predictions_xg_train %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "train")) %>%
mutate(model = "decision_xg")
auc_results <-
rbind(auc_tree, auc_rf, auc_xg)
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_col(position = "dodge") +
labs(x = "Model", y = "AUC results")
auc_results
# Make a search grid for the k-parameter
xg_grid <-
grid_latin_hypercube(
tree_depth(),
loss_reduction(),
sample_size = sample_prop(),
learn_rate(),
finalize(mtry(), spam_train),
min_n(),
size = 10
)
# Calculate the cross-validated AUC for all the k's in the grid
xg_tune_result <-
tune_grid(
xg_workflow,
resamples = spam_folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE)
)
# Which parameter combination is the best?
xg_tune_result %>%
select_best(metric = "roc_auc")
# Put the best parameters in the workflow
xg_tuned <-
finalize_workflow(
xg_workflow,
parameters = xg_tune_result %>% select_best(metric = "roc_auc")
)
# Fit the model
fitted_xg <-
xg_tuned %>%
fit(data = spam_train)
# Predict the train and test data
predictions_xg_test <-
fitted_xg %>%
predict(new_data = spam_test,
type = "prob") %>%
mutate(truth = spam_test$spam)
predictions_xg_train <-
fitted_xg %>%
predict(new_data = spam_train,
type = "prob") %>%
mutate(truth = spam_train$spam)
# Calculate the AUC
auc_xg <-
predictions_xg_test %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "test") %>%
bind_rows(predictions_xg_train %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "train")) %>%
mutate(model = "decision_xg")
auc_results <-
rbind(auc_tree, auc_rf, auc_xg)
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_col(position = "dodge") +
labs(x = "Model", y = "AUC results")
auc_results
auc_results
#Specify the decision tree
xg_mod <-
boost_tree(
trees = 100,
tree_depth = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry = tune(),
min_n = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost")
# Set up the workflow
xg_workflow <-
workflow() %>%
add_model(xg_mod) %>%
add_recipe(spam_recipe)
# Make a search grid for the k-parameter
xg_grid <-
grid_latin_hypercube(
tree_depth(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), spam_train),
min_n(),
size = 10
)
# Calculate the cross-validated AUC for all the k's in the grid
xg_tune_result <-
tune_grid(
xg_workflow,
resamples = spam_folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE)
)
# Which parameter combination is the best?
xg_tune_result %>%
select_best(metric = "roc_auc")
# Put the best parameters in the workflow
xg_tuned <-
finalize_workflow(
xg_workflow,
parameters = xg_tune_result %>% select_best(metric = "roc_auc")
)
# Fit the model
fitted_xg <-
xg_tuned %>%
fit(data = spam_train)
# Predict the train and test data
predictions_xg_test <-
fitted_xg %>%
predict(new_data = spam_test,
type = "prob") %>%
mutate(truth = spam_test$spam)
predictions_xg_train <-
fitted_xg %>%
predict(new_data = spam_train,
type = "prob") %>%
mutate(truth = spam_train$spam)
# Calculate the AUC
auc_xg <-
predictions_xg_test %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "test") %>%
bind_rows(predictions_xg_train %>%
roc_auc(truth, .pred_0) %>%
mutate(where = "train")) %>%
mutate(model = "decision_xg")
auc_results <-
rbind(auc_tree, auc_rf, auc_xg)
auc_results %>%
ggplot(aes(x = model, .estimate, fill = where)) +
geom_col(position = "dodge") +
labs(x = "Model", y = "AUC results")
auc_results
# Assignment 1:
library(tweedie)
library(ggplot2)
install.packages("tictoc")
library(tictoc)
# Assignment 1:
library(tweedie)
library(ggplot2)
library(tictoc)
simTweedieTest <-
function(N){
t.test(
rtweedie(N, mu=10000, phi=100, power=1.9),
mu=10000
)$p.value
}
# Assignment 2:
MTweedieTests <-
function(N,M,sig){
sum(replicate(M,simTweedieTest(N)) < sig)/M
}
# Assignment 3:
df <-
expand.grid(
N = c(10,100,1000,5000, 10000),
M = 1000,
share_reject = NA)
for(i in 1:nrow(df)){
df$share_reject[i] <-
MTweedieTests(
N=df$N[i],
M=df$M[i],
sig=.05)
}
library(magrittr)
library(tidyverse)
simDat <-
function(N, type, mu) {
if (type == "tweedie") {
return(rtweedie(
N,
mu = mu,
phi = 100,
power = 1.9
))
}
if (type == "normal") {
return(rnorm(N, mean = mu))
}
else{
stop("invalid distribution")
}
}
# Next, the test. Note, we use mu two places:
# both for the data simulation and as the null.
simTest <-
function(N, type, mu) {
t.test(simDat(N = N,
type = type,
mu = mu),
mu = mu)$p.value
}
# Running many tests is almost the same as before.
# Here the mean is hard coded in, as we're not
# going to change it.
MTests <-
function(N, M, type, sig) {
sum(replicate(M,
simTest(
N = N,
type =
type,
mu =
10000
)) < sig) / M
}
# We can now repeat the same analysis as before,
# but for both the tweedie and the normal:
df <-
expand.grid(
N = c(10, 100, 1000, 5000),
M = 1000,
type = c("tweedie", "normal"),
share_reject = NA
) %>%
as_tibble()
for (i in 1:nrow(df)) {
print(i)
df$share_reject[i] <-
MTests(df$N[i],
df$M[i],
df$type[i],
.05)
}
# As you see, with normally distributed data, N can
# be very small and the t-test is fine. With a tweedie,
# "large enough" can be many thousands. If we try
# different distributions or parameterizations, we might
# also get different results.
df %>%
ggplot2::ggplot(aes(x = log(N), y = share_reject, col = type)) +
geom_line() +
geom_hline(yintercept = .05) +
theme_bw()
setwd("C:/Users/tords/OneDrive/Skrivebord/BAN400/Assignments Part 2/parallel-computing-TordBa/scripts")
setwd("C:/Users/tords/OneDrive/Skrivebord/BAN400/Assignments Part 2/parallel-computing-TordBa")
library(tictoc)
source("scripts/hw4_solution.R")
tic.clearlog()
tic("hw4_solution script")
toc(log = TRUE)
tic.clearlog()
tic("hw4_solution script")
source("scripts/hw4_solution.R")
toc(log = TRUE)
install.packages("doParallel")
library(doParallel)
printTicTocLog <-
function() {
tic.log() %>%
unlist %>%
tibble(logvals = .) %>%
separate(logvals,
sep = ":",
into = c("Function type", "log")) %>%
mutate(log = str_trim(log)) %>%
separate(log,
sep = " ",
into = c("Seconds"),
extra = "drop")
}
library(tictoc)
printTicTocLog <-
function() {
tic.log() %>%
unlist %>%
tibble(logvals = .) %>%
separate(logvals,
sep = ":",
into = c("Function type", "log")) %>%
mutate(log = str_trim(log)) %>%
separate(log,
sep = " ",
into = c("Seconds"),
extra = "drop")
}
tic.clearlog()
library(tictoc)
printTicTocLog <-
function() {
tic.log() %>%
unlist %>%
tibble(logvals = .) %>%
separate(logvals,
sep = ":",
into = c("Function type", "log")) %>%
mutate(log = str_trim(log)) %>%
separate(log,
sep = " ",
into = c("Seconds"),
extra = "drop")
}
tic.clearlog()
tic("hw4_solution script")
source("scripts/hw4_solution.R")
toc(log = TRUE)
tic(paste0("hw4_solution2 script", Cores, "cores"))
tic(paste0("hw4_solution2 script"))
tic.clearlog()
tic("hw4_solution script")
source("scripts/hw4_solution.R")
toc(log = TRUE)
tic(paste0("hw4_solution2 script"))
source("scripts/hw4_solution2.R")
toc(log = TRUE)
printTicTocLog() %>%
knitr::kable()
library(tictoc)
printTicTocLog <-
function() {
tic.log() %>%
unlist %>%
tibble(logvals = .) %>%
separate(logvals,
sep = ":",
into = c("Function type", "log")) %>%
mutate(log = str_trim(log)) %>%
separate(log,
sep = " ",
into = c("Seconds"),
extra = "drop")
}
# Task 1
tic.clearlog()
tic("hw4_solution script")
source("scripts/hw4_solution.R")
toc(log = TRUE)
# Task 2
tic(paste0("hw4_solution2 script"))
source("scripts/hw4_solution2.R")
toc(log = TRUE)
# Task 3
tic(paste0("hw4_solution3 script"))
source("scripts/hw4_solution3.R")
# Task 2
tic(paste0("hw4_solution2 script"))
source("scripts/hw4_solution2.R")
toc(log = TRUE)
